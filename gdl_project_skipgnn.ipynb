{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gdl_project.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xttum9YLdEpu",
        "outputId": "e79cd36f-6da6-4be3-e485-a38ce18d50fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vcnG396dCf_",
        "outputId": "5d9efb27-4485-4543-e80a-40f80a604e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 11 07:05:21 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  \n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7zxht7GdQPZ",
        "outputId": "f8b6c84e-e67b-4384-841b-c4215747a0f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 9.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.13\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 7.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-6rzna6u6\n",
            "  Running command git clone -q https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-6rzna6u6\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (3.0.8)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric==2.0.5) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric==2.0.5) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric==2.0.5) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.5-py3-none-any.whl size=639362 sha256=b910a2f382e244bf54ae15aa3ec5a2fd5961b9f02c438d72812bcaaed998559c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-oaagl9rd/wheels/85/c9/07/7936efecad79b906348a7e9fb644d914160544efa9aa7f4b2b\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ogb"
      ],
      "metadata": {
        "id": "8rSoCJq9dTwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81068d18-dd7f-4bb9-d04c-d8948fbb12e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ogb\n",
            "  Downloading ogb-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▏                           | 10 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 20 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 30 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 40 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 51 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 61 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 71 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 78 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.64.0)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=8cbceeaec780904d88d692cc2298543ae97a285761719c6e0389e22ff85054d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.3 outdated-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_sparse\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.utils import negative_sampling, to_networkx\n",
        "from typing import Union, Tuple\n",
        "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "import torch_geometric.nn.conv as ccgeo\n",
        "from torch_geometric.nn.conv import MessagePassing"
      ],
      "metadata": {
        "id": "dIx82qrwdV2r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.utils import negative_sampling, to_networkx\n",
        "from typing import Union, Tuple\n",
        "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "import torch_geometric.nn.conv as ccgeo\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch.nn.modules.module import Module\n",
        "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
        "import pynvml as pynv\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch.nn as nn\n",
        "import scipy.sparse as sp\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import math\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_sparse\n",
        "##### SCRIPT STARTS HERE #####\n",
        "#!usr/bin/bash python\n",
        "# instead of nx.shortest_path_length defaults to dijkstra's shortest path, could try Bellman-Ford or etc. Follow: https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.shortest_path_length.html\n",
        "\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "\n",
        "def skipgnn_preprocessing(edges, num_nodes):\n",
        "    edges = edges.T\n",
        "    features = np.eye(num_nodes)\n",
        "\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                    shape=(num_nodes, num_nodes),\n",
        "                    dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "\n",
        "    #create skip graph\n",
        "    adj2 = adj.dot(adj)\n",
        "    adj2 = adj2.sign()\n",
        "\n",
        "    adj2 = normalize_adj(adj2)\n",
        "    adj2 = sparse_mx_to_torch_sparse_tensor(adj2)\n",
        "\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "\n",
        "    #normalize original graph\n",
        "    adj = normalize_adj(adj)\n",
        "\n",
        "    features = torch.FloatTensor(features)\n",
        "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "\n",
        "    return adj, adj2, features\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, runs, info=None):\n",
        "        self.info = info\n",
        "        self.results = [[] for _ in range(runs)]\n",
        "\n",
        "    def add_result(self, run, result):\n",
        "        assert len(result) == 2\n",
        "        assert run >= 0 and run < len(self.results)\n",
        "        self.results[run].append(result)\n",
        "\n",
        "    def print_statistics(self, run=None):\n",
        "        if run is not None:\n",
        "            result = 100 * torch.tensor(self.results[run])\n",
        "            print(result)\n",
        "            argmax = result[:, 0].argmax().item()\n",
        "            print(f'Run {run + 1:02d}:')\n",
        "            print(f'Highest Valid: {result[:, 0].max():.2f}')\n",
        "            print(f'Highest Test: {result[:, 1].max():.2f}')\n",
        "            print(f'   Final Test: {result[argmax, 1]:.2f}')\n",
        "        else:\n",
        "            result = 100 * torch.tensor(self.results)\n",
        "            best_results = []\n",
        "            for r in result:\n",
        "                valid = r[:, 0].max().item()\n",
        "                test = r[r[:, 0].argmax(), 1].item()\n",
        "                best_results.append((valid, test))\n",
        "            best_result = torch.tensor(best_results)\n",
        "            print(f'All runs:')\n",
        "            r = best_result[:, 0]\n",
        "            print(f'Highest Valid: {r.mean():.4f} ± {r.std():.4f}')\n",
        "            r = best_result[:, 1]\n",
        "            print(f'   Final Test: {r.mean():.4f} ± {r.std():.4f}')\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # if input.shape[0] == self.weight.shape[0]:\n",
        "        #     support = torch.mul(input, self.weight)\n",
        "        # else:\n",
        "        #     support = torch.mm(input, self.weight)\n",
        "\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "class SkipGNN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid1, nhid2, nhid_decode1, dropout):\n",
        "        super(SkipGNN, self).__init__()\n",
        "        \n",
        "        # original graph\n",
        "        self.o_gc1 = GraphConvolution(nfeat, nhid1)\n",
        "        self.o_gc2 = GraphConvolution(nhid1, nhid2)\n",
        "        \n",
        "        # original graph for skip update\n",
        "        self.o_gc1_s = GraphConvolution(nhid1, nhid1)\n",
        "        \n",
        "        #skip graph\n",
        "        self.s_gc1 = GraphConvolution(nfeat, nhid1)\n",
        "        \n",
        "        #skip graph for original update\n",
        "        self.s_gc1_o = GraphConvolution(nfeat, nhid1)\n",
        "        self.s_gc2_o = GraphConvolution(nhid1, nhid2)\n",
        "       \n",
        "        self.dropout = dropout\n",
        "        \n",
        "    def forward(self, x, o_adj, s_adj):\n",
        "        o_x = F.relu(self.o_gc1(x, o_adj) + self.s_gc1_o(x, s_adj))       \n",
        "        s_x = F.relu(self.s_gc1(x, s_adj) + self.o_gc1_s(o_x, o_adj))\n",
        "        \n",
        "        o_x = F.dropout(o_x, self.dropout, training = self.training)\n",
        "        s_x = F.dropout(s_x, self.dropout, training = self.training)\n",
        "        \n",
        "        x = self.o_gc2(o_x, o_adj) + self.s_gc2_o(s_x, s_adj)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class LinkPredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(LinkPredictor, self).__init__()\n",
        "\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
        "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for lin in self.lins:\n",
        "            lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x_i, x_j):\n",
        "        x = x_i * x_j\n",
        "        for lin in self.lins[:-1]:\n",
        "            x = lin(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lins[-1](x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def train(model, predictor, x, adj, adj_2, split_edge, optimizer, device, batch_size, edge_index):\n",
        "    model.train()\n",
        "    predictor.train()\n",
        "    pos_train_edge = split_edge['train']['edge'].to(device)\n",
        "\n",
        "    total_loss = total_examples = 0\n",
        "    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size, shuffle=True):\n",
        "        optimizer.zero_grad()\n",
        "        # h will be embeddings\n",
        "        h = model(x, adj, adj_2)\n",
        "        # print(\"Model output shape:\",h.shape)\n",
        "        # edge: we get a sampling of positive edge connections between nodes by using perm(list of nodes for the batch we know are positive)\n",
        "        edge = pos_train_edge[perm].t()\n",
        "        # we pass the edge list inside embedding h\n",
        "        # we devide in edge[0] and edge[1] to devide the connected from->to\n",
        "        pos_out = predictor(h[edge[0]], h[edge[1]])\n",
        "        pos_loss = -torch.log(pos_out + 1e-15).mean()\n",
        "        # neg_sampling(we get negative edges from dataset) REAL ONES! they use the full dataset!!! VERY GOOD :)\n",
        "        edge = negative_sampling(edge_index, num_nodes=x.size(0),\n",
        "                                 num_neg_samples=perm.size(0), method='dense')\n",
        "\n",
        "        neg_out = predictor(h[edge[0]], h[edge[1]])\n",
        "        neg_loss = -torch.log(1 - neg_out + 1e-15).mean()\n",
        "\n",
        "        loss = pos_loss + neg_loss\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(x, 1.0)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        num_examples = pos_out.size(0)\n",
        "        total_loss += loss.item() * num_examples\n",
        "        total_examples += num_examples\n",
        "\n",
        "    return total_loss / total_examples\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, predictor, x, adj, adj_2, split_edge, evaluator, batch_size):\n",
        "    model.eval()\n",
        "    predictor.eval()\n",
        "\n",
        "    h = model(x, adj, adj_2)\n",
        "\n",
        "    pos_valid_edge = split_edge['valid']['edge'].to(x.device)\n",
        "    neg_valid_edge = split_edge['valid']['edge_neg'].to(x.device)\n",
        "    pos_test_edge = split_edge['test']['edge'].to(x.device)\n",
        "    neg_test_edge = split_edge['test']['edge_neg'].to(x.device)\n",
        "\n",
        "    pos_valid_preds = []\n",
        "    for perm in DataLoader(range(pos_valid_edge.size(0)), batch_size):\n",
        "        edge = pos_valid_edge[perm].t()\n",
        "        pos_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    pos_valid_pred = torch.cat(pos_valid_preds, dim=0)\n",
        "\n",
        "    neg_valid_preds = []\n",
        "    for perm in DataLoader(range(neg_valid_edge.size(0)), batch_size):\n",
        "        edge = neg_valid_edge[perm].t()\n",
        "        neg_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n",
        "\n",
        "    pos_test_preds = []\n",
        "    for perm in DataLoader(range(pos_test_edge.size(0)), batch_size):\n",
        "        edge = pos_test_edge[perm].t()\n",
        "        pos_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    pos_test_pred = torch.cat(pos_test_preds, dim=0)\n",
        "\n",
        "    neg_test_preds = []\n",
        "    for perm in DataLoader(range(neg_test_edge.size(0)), batch_size):\n",
        "        edge = neg_test_edge[perm].t()\n",
        "        neg_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    neg_test_pred = torch.cat(neg_test_preds, dim=0)\n",
        "\n",
        "    results = {}\n",
        "    for K in [20, 50, 100]:\n",
        "        evaluator.K = K\n",
        "        valid_hits = evaluator.eval({\n",
        "            'y_pred_pos': pos_valid_pred,\n",
        "            'y_pred_neg': neg_valid_pred,\n",
        "        })[f'hits@{K}']\n",
        "        test_hits = evaluator.eval({\n",
        "            'y_pred_pos': pos_test_pred,\n",
        "            'y_pred_neg': neg_test_pred,\n",
        "        })[f'hits@{K}']\n",
        "\n",
        "        results[f'Hits@{K}'] = (valid_hits, test_hits)\n",
        "\n",
        "    return results\n",
        "    \n",
        "\n",
        "def print_gpu_utilization(position):\n",
        "    print(position)\n",
        "    os.system(\"nvidia-smi\")\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Link_Pred_DDI')\n",
        "    parser.add_argument('--device', type=str, default=\"cuda:0\")\n",
        "    parser.add_argument('--num_layers', type=int, default=2)\n",
        "    parser.add_argument('--num_samples', type=int, default=5)\n",
        "    parser.add_argument('--node_emb', type=int, default=256)\n",
        "    parser.add_argument('--hidden_channels', type=int, default=256)\n",
        "    parser.add_argument('--spd_size', type=int, default=500)\n",
        "    # parser.add_argument('--batch_size', type=int, default=1) \n",
        "    # TODO\n",
        "    parser.add_argument('--batch_size', type=int, default=64 * 1024)\n",
        "    parser.add_argument('--lr', type=float, default=0.003)\n",
        "    parser.add_argument('--epochs', type=int, default=400)\n",
        "    parser.add_argument('--log_steps', type=int, default=1)\n",
        "    parser.add_argument('--eval_steps', type=int, default=1)\n",
        "    parser.add_argument('--runs', type=int, default=1)\n",
        "    parser.add_argument('--hidden1', type=int, default=256,\n",
        "                        help='Number of hidden units for encoding layer 1.')\n",
        "    parser.add_argument('--hidden2', type=int, default=256,\n",
        "                        help='Number of hidden units for encoding layer 2.')\n",
        "    parser.add_argument('--hidden_decode1', type=int, default=16,\n",
        "                        help='Number of hidden units for decoding layer 1.')\n",
        "    parser.add_argument('--dropout', type=float, default=0.3,\n",
        "                        help='Dropout rate (1 - keep probability).')\n",
        "    args = parser.parse_args()\n",
        "    print(args)\n",
        "    device = args.device if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "    \n",
        "\n",
        "    dataset = PygLinkPropPredDataset(name='ogbl-ddi')\n",
        "    data = dataset[0]\n",
        "    print(data.size())\n",
        "    edge_index = data.edge_index.to(device)\n",
        "    print_gpu_utilization(\"Start\")\n",
        "\n",
        "    split_edge = dataset.get_edge_split()\n",
        "\n",
        "    # ---\n",
        "    # create adj matrices required for SkipGNN\n",
        "    edges = data.edge_index\n",
        "    # edges = split_edge['train']['edge']\n",
        "    adj, adj2, features = skipgnn_preprocessing(edges, data['num_nodes'])\n",
        "    adj, adj2, features = adj.to(device), adj2.to(device), features\n",
        "    \n",
        "    # ---\n",
        "\n",
        "    model = SkipGNN(nfeat=features.shape[1],\n",
        "            nhid1=args.hidden1,\n",
        "            nhid2=args.hidden2,\n",
        "            nhid_decode1 = args.hidden_decode1,\n",
        "            dropout=args.dropout).to(device)\n",
        "\n",
        "    emb = torch.nn.Embedding(data.num_nodes, args.node_emb).to(device)\n",
        "    predictor = LinkPredictor(args.hidden_channels, args.hidden_channels, 1,\n",
        "                              args.num_layers+1, args.dropout).to(device)\n",
        "\n",
        "    print('Number of parameters:',\n",
        "          sum(p.numel() for p in list(model.parameters()) +\n",
        "          list(predictor.parameters()) + list(emb.parameters())))\n",
        "\n",
        "    evaluator = Evaluator(name='ogbl-ddi')\n",
        "    loggers = {\n",
        "        'Hits@20': Logger(args.runs, args),\n",
        "        'Hits@50': Logger(args.runs, args),\n",
        "        'Hits@100': Logger(args.runs, args),\n",
        "    }\n",
        "    for run in range(args.runs):\n",
        "        torch.cuda.empty_cache()\n",
        "        random.seed(run)\n",
        "        torch.manual_seed(run)\n",
        "        torch.nn.init.xavier_uniform_(emb.weight)\n",
        "        # model.reset_parameters()\n",
        "        predictor.reset_parameters()\n",
        "        optimizer = torch.optim.Adam(\n",
        "            list(model.parameters()) + list(emb.parameters()) + list(predictor.parameters()), lr=args.lr)\n",
        "        \n",
        "        for epoch in range(1, 1 + args.epochs):\n",
        "            loss = train(model, predictor, emb.weight, adj, adj2, split_edge,\n",
        "                         optimizer,device, args.batch_size, edge_index)\n",
        "            if epoch % args.eval_steps == 0:\n",
        "                results = test(model, predictor, emb.weight, adj, adj2, split_edge, evaluator, args.batch_size)\n",
        "                for key, result in results.items():\n",
        "                    loggers[key].add_result(run, result)\n",
        "\n",
        "                if epoch % args.log_steps == 0:\n",
        "                    for key, result in results.items():\n",
        "                        valid_hits, test_hits = result\n",
        "                        print(key)\n",
        "                        print(f'Run: {run + 1:02d}, '\n",
        "                              f'Epoch: {epoch:02d}, '\n",
        "                              f'Loss: {loss:.4f}, '\n",
        "                              f'Valid: {100 * valid_hits:.2f}%, '\n",
        "                              f'Test: {100 * test_hits:.2f}%')\n",
        "                    print('---')\n",
        "        torch.save(model,\"save.pth\")\n",
        "        for key in loggers.keys():\n",
        "            print(key)\n",
        "            loggers[key].print_statistics(run)\n",
        "\n",
        "    for key in loggers.keys():\n",
        "        print(key)\n",
        "        loggers[key].print_statistics()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh5ABtPxd7wq",
        "outputId": "eb04ba5e-c280-4f6b-ee7e-dd48a9c9d518"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --node_emb 4267 --hidden_channels 256 --num_layers 2 --epochs 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycGX6WGQeFNz",
        "outputId": "ecef9b0d-4f68-436e-edc6-b3c8990a264c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=65536, device='cuda:0', dropout=0.3, epochs=1000, eval_steps=1, hidden1=256, hidden2=256, hidden_channels=256, hidden_decode1=16, log_steps=1, lr=0.003, node_emb=4267, num_layers=2, num_samples=5, runs=1, spd_size=500)\n",
            "(4267, 4267)\n",
            "Start\n",
            "Wed May 11 07:10:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    33W / 250W |    969MiB / 16280MiB |      4%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Number of parameters: 21814330\n",
            "Hits@20\n",
            "Run: 01, Epoch: 01, Loss: 1.2446, Valid: 3.72%, Test: 7.35%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 01, Loss: 1.2446, Valid: 6.50%, Test: 10.13%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 01, Loss: 1.2446, Valid: 9.60%, Test: 12.60%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 02, Loss: 0.8560, Valid: 5.48%, Test: 4.57%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 02, Loss: 0.8560, Valid: 7.28%, Test: 8.54%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 02, Loss: 0.8560, Valid: 10.81%, Test: 11.78%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 03, Loss: 0.7097, Valid: 14.94%, Test: 5.97%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 03, Loss: 0.7097, Valid: 19.33%, Test: 15.12%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 03, Loss: 0.7097, Valid: 23.28%, Test: 19.33%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 04, Loss: 0.6451, Valid: 9.11%, Test: 6.09%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 04, Loss: 0.6451, Valid: 13.67%, Test: 17.18%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 04, Loss: 0.6451, Valid: 21.01%, Test: 22.29%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 05, Loss: 0.6121, Valid: 12.44%, Test: 13.45%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 05, Loss: 0.6121, Valid: 18.48%, Test: 20.04%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 05, Loss: 0.6121, Valid: 24.27%, Test: 25.76%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 06, Loss: 0.5789, Valid: 18.08%, Test: 15.06%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 06, Loss: 0.5789, Valid: 26.05%, Test: 23.51%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 06, Loss: 0.5789, Valid: 32.02%, Test: 30.79%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 07, Loss: 0.5314, Valid: 26.09%, Test: 20.30%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 07, Loss: 0.5314, Valid: 33.20%, Test: 30.48%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 07, Loss: 0.5314, Valid: 39.01%, Test: 36.91%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 08, Loss: 0.4929, Valid: 23.52%, Test: 20.80%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 08, Loss: 0.4929, Valid: 34.02%, Test: 30.36%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 08, Loss: 0.4929, Valid: 41.92%, Test: 37.84%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 09, Loss: 0.4694, Valid: 27.13%, Test: 21.42%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 09, Loss: 0.4694, Valid: 34.10%, Test: 30.43%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 09, Loss: 0.4694, Valid: 40.37%, Test: 37.44%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 10, Loss: 0.4535, Valid: 18.01%, Test: 12.77%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 10, Loss: 0.4535, Valid: 27.66%, Test: 21.92%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 10, Loss: 0.4535, Valid: 38.26%, Test: 34.76%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 11, Loss: 0.4384, Valid: 18.52%, Test: 14.64%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 11, Loss: 0.4384, Valid: 27.79%, Test: 24.41%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 11, Loss: 0.4384, Valid: 37.72%, Test: 35.32%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 12, Loss: 0.4132, Valid: 24.82%, Test: 14.13%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 12, Loss: 0.4132, Valid: 35.75%, Test: 25.82%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 12, Loss: 0.4132, Valid: 45.47%, Test: 39.19%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 13, Loss: 0.3887, Valid: 28.60%, Test: 23.91%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 13, Loss: 0.3887, Valid: 40.41%, Test: 35.72%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 13, Loss: 0.3887, Valid: 48.61%, Test: 44.54%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 14, Loss: 0.3773, Valid: 30.56%, Test: 20.58%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 14, Loss: 0.3773, Valid: 39.85%, Test: 32.36%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 14, Loss: 0.3773, Valid: 48.45%, Test: 42.35%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 15, Loss: 0.3636, Valid: 19.46%, Test: 15.99%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 15, Loss: 0.3636, Valid: 27.84%, Test: 26.64%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 15, Loss: 0.3636, Valid: 38.76%, Test: 37.20%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 16, Loss: 0.3552, Valid: 21.42%, Test: 11.37%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 16, Loss: 0.3552, Valid: 34.80%, Test: 25.15%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 16, Loss: 0.3552, Valid: 43.06%, Test: 36.76%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 17, Loss: 0.3579, Valid: 34.07%, Test: 18.70%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 17, Loss: 0.3579, Valid: 46.53%, Test: 34.37%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 17, Loss: 0.3579, Valid: 54.18%, Test: 45.04%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 18, Loss: 0.3370, Valid: 28.27%, Test: 15.70%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 18, Loss: 0.3370, Valid: 39.03%, Test: 34.15%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 18, Loss: 0.3370, Valid: 49.14%, Test: 45.66%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 19, Loss: 0.3259, Valid: 23.43%, Test: 9.82%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 19, Loss: 0.3259, Valid: 37.86%, Test: 26.05%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 19, Loss: 0.3259, Valid: 48.59%, Test: 40.72%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 20, Loss: 0.3189, Valid: 30.40%, Test: 16.20%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 20, Loss: 0.3189, Valid: 42.51%, Test: 32.99%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 20, Loss: 0.3189, Valid: 53.20%, Test: 47.32%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 21, Loss: 0.3110, Valid: 22.67%, Test: 13.47%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 21, Loss: 0.3110, Valid: 38.93%, Test: 31.66%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 21, Loss: 0.3110, Valid: 49.63%, Test: 43.24%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 22, Loss: 0.3025, Valid: 25.65%, Test: 19.88%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 22, Loss: 0.3025, Valid: 39.78%, Test: 34.80%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 22, Loss: 0.3025, Valid: 50.70%, Test: 47.71%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 23, Loss: 0.3079, Valid: 23.76%, Test: 15.41%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 23, Loss: 0.3079, Valid: 37.42%, Test: 29.17%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 23, Loss: 0.3079, Valid: 47.03%, Test: 44.08%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 24, Loss: 0.2954, Valid: 24.01%, Test: 13.42%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 24, Loss: 0.2954, Valid: 38.00%, Test: 29.60%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 24, Loss: 0.2954, Valid: 49.56%, Test: 43.69%\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}
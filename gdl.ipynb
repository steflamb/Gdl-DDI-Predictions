{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gdl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVruFNRjODs9",
        "outputId": "bef2d733-45fb-4923-f66b-0e09f48ad06f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrk-ZyzEUcrG",
        "outputId": "69f3cf0f-0a92-4175-e100-21bcf9a77e1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 27 09:00:48 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  \n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xu8offvO2Uj",
        "outputId": "9708caf3-1a39-47d4-d68f-762213163b29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.13\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-mkyge01y\n",
            "  Running command git clone -q https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-mkyge01y\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (3.0.8)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric==2.0.5) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric==2.0.5) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric==2.0.5) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.5-py3-none-any.whl size=630528 sha256=8c340018500b992435bb787b88f475a4eefa63ef5363a1b060a0105fd70d8696\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v2mx3m5q/wheels/85/c9/07/7936efecad79b906348a7e9fb644d914160544efa9aa7f4b2b\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ogb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHe297hDPBgd",
        "outputId": "0688b698-91e3-4f2b-aa8a-18132561b20b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ogb\n",
            "  Downloading ogb-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▏                           | 10 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 20 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 30 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 51 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 71 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 78 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.11.0+cu113)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.64.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2022.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=de89be43b8a0e4f345552b9dcc7500edee9ca4bab43eb1863d89ec2148b78452\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.3 outdated-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D3jeEFJTOKRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_sparse\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.utils import negative_sampling, to_networkx\n",
        "from typing import Union, Tuple\n",
        "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "import torch_geometric.nn.conv as ccgeo\n",
        "from torch_geometric.nn.conv import MessagePassing\n"
      ],
      "metadata": {
        "id": "I4uuWdW8OMS_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUEHxr8ZN2XC",
        "outputId": "254f4250-4b8e-4090-cbfc-1c4bab705822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.utils import negative_sampling, to_networkx\n",
        "from typing import Union, Tuple\n",
        "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "import torch_geometric.nn.conv as ccgeo\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
        "import pynvml as pynv\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_sparse\n",
        "##### SCRIPT STARTS HERE #####\n",
        "#!usr/bin/bash python\n",
        "\n",
        "def get_spd_matrix(G, S, max_spd=5): # nx_graph is the complete graph networkx DS, node_subset is np.random.choice(nx_graph.number_of_nodes(), size=500, replace=False)\n",
        "    r\"\"\"S parameter draws (In the original version, get_spd_matrix is called in line ~363ish) 500 random samples within range 0 to G.number_of_nodes(),\n",
        "    (again , in one instance, this range is 0 to 4267)\n",
        "    \"\"\"\n",
        "    print(len(G))\n",
        "    spd_matrix = np.zeros((G.number_of_nodes(), len(S)), dtype=np.float32) # returns a 0-matrix of (4267, 500)\n",
        "    print(\"spd_for\")\n",
        "    for i, node_S in enumerate(S):                                          # i is index of for loop, node_S is the i-th element of S (current node)\n",
        "        for node, length in nx.shortest_path_length(G, source=node_S).items():  # iterate over the Graph where source is node_S, let length denote the distance from node_S to current node\n",
        "            spd_matrix[node, i] = min(length, max_spd)                          # refill element spd[node,i] with min(length, max_spd)  \n",
        "    print(\"spd_end\")\n",
        "    return spd_matrix                                                           # conclusion: spd elements will contain at most 5\n",
        "\n",
        "# instead of nx.shortest_path_length defaults to dijkstra's shortest path, could try Bellman-Ford or etc. Follow: https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.shortest_path_length.html\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, runs, info=None):\n",
        "        self.info = info\n",
        "        self.results = [[] for _ in range(runs)]\n",
        "\n",
        "    def add_result(self, run, result):\n",
        "        assert len(result) == 2\n",
        "        assert run >= 0 and run < len(self.results)\n",
        "        self.results[run].append(result)\n",
        "\n",
        "    def print_statistics(self, run=None):\n",
        "        if run is not None:\n",
        "            result = 100 * torch.tensor(self.results[run])\n",
        "            print(result)\n",
        "            argmax = result[:, 0].argmax().item()\n",
        "            print(f'Run {run + 1:02d}:')\n",
        "            print(f'Highest Valid: {result[:, 0].max():.2f}')\n",
        "            print(f'Highest Test: {result[:, 1].max():.2f}')\n",
        "            print(f'   Final Test: {result[argmax, 1]:.2f}')\n",
        "        else:\n",
        "            result = 100 * torch.tensor(self.results)\n",
        "            best_results = []\n",
        "            for r in result:\n",
        "                valid = r[:, 0].max().item()\n",
        "                test = r[r[:, 0].argmax(), 1].item()\n",
        "                best_results.append((valid, test))\n",
        "            best_result = torch.tensor(best_results)\n",
        "            print(f'All runs:')\n",
        "            r = best_result[:, 0]\n",
        "            print(f'Highest Valid: {r.mean():.4f} ± {r.std():.4f}')\n",
        "            r = best_result[:, 1]\n",
        "            print(f'   Final Test: {r.mean():.4f} ± {r.std():.4f}')\n",
        "\n",
        "\n",
        "class SAGEConv(MessagePassing):\n",
        "    r\"\"\"The GraphSAGE operator from the `\"Inductive Representation Learning on\n",
        "    Large Graphs\" <https://arxiv.org/abs/1706.02216>`_ paper\n",
        "\n",
        "    .. math::\n",
        "        \\mathbf{x}^{\\prime}_i = \\mathbf{W}_1 \\mathbf{x}_i + \\mathbf{W}_2 \\cdot\n",
        "        \\mathrm{mean}_{j \\in \\mathcal{N(i)}} \\mathbf{x}_j\n",
        "\n",
        "    Args:\n",
        "        in_channels (int or tuple): Size of each input sample. A tuple\n",
        "            corresponds to the sizes of source and target dimensionalities.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        normalize (bool, optional): If set to :obj:`True`, output features\n",
        "            will be :math:`\\ell_2`-normalized, *i.e.*,\n",
        "            :math:`\\frac{\\mathbf{x}^{\\prime}_i}\n",
        "            {\\| \\mathbf{x}^{\\prime}_i \\|_2}`.\n",
        "            (default: :obj:`False`)\n",
        "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
        "            not add transformed root node features to the output.\n",
        "            (default: :obj:`True`)\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
        "                 out_channels: int, normalize: bool = False,\n",
        "                 root_weight: bool = True,\n",
        "                 bias: bool = True, **kwargs):  # yapf: disable\n",
        "        kwargs.setdefault('aggr', 'mean')\n",
        "        super(SAGEConv, self).__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.normalize = normalize\n",
        "        self.root_weight = root_weight\n",
        "\n",
        "        if isinstance(in_channels, int):\n",
        "            in_channels = (in_channels, in_channels)\n",
        "\n",
        "        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n",
        "        if self.root_weight:\n",
        "            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin_l.reset_parameters()\n",
        "        if self.root_weight:\n",
        "            self.lin_r.reset_parameters()\n",
        "\n",
        "\n",
        "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
        "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        if isinstance(x, Tensor):\n",
        "            x: OptPairTensor = (x, x)\n",
        "\n",
        "        # Node and edge feature dimensionalites need to match.\n",
        "        if isinstance(edge_index, Tensor):\n",
        "            assert edge_attr is not None\n",
        "            assert x[0].size(-1) == edge_attr.size(-1)\n",
        "        elif isinstance(edge_index, SparseTensor):\n",
        "            assert x[0].size(-1) == edge_index.size(-1)\n",
        "\n",
        "        # propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\n",
        "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n",
        "        out = self.lin_l(out)\n",
        "\n",
        "        x_r = x[1]\n",
        "        if self.root_weight and x_r is not None:\n",
        "            out += self.lin_r(x_r)\n",
        "\n",
        "        if self.normalize:\n",
        "            out = F.normalize(out, p=2., dim=-1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
        "        return F.relu(x_j + edge_attr)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
        "                                   self.out_channels)\n",
        "\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
        "        super(GraphSAGE,self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t, edge_attr, emb_ea):\n",
        "        edge_attr = torch.mm(edge_attr, emb_ea)\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, adj_t, edge_attr)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t, edge_attr)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LinkPredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(LinkPredictor, self).__init__()\n",
        "\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
        "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for lin in self.lins:\n",
        "            lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x_i, x_j):\n",
        "        x = x_i * x_j\n",
        "        for lin in self.lins[:-1]:\n",
        "            x = lin(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lins[-1](x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def train(model, predictor, edge_attr, x, emb_ea, adj_t, split_edge, optimizer,device, batch_size):\n",
        "    edge_index = adj_t\n",
        "    model.train()\n",
        "    predictor.train()\n",
        "    pos_train_edge = split_edge['train']['edge'].to(device)\n",
        "\n",
        "    total_loss = total_examples = 0\n",
        "    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size, shuffle=True):\n",
        "        torch.cuda.empty_cache()\n",
        "        optimizer.zero_grad()\n",
        "        # h will be embeddings\n",
        "        # x are the embeddings before\n",
        "        # adj_t is edge index (All REAL edges)\n",
        "        # edge_attr is x3 sampling of SPD\n",
        "        # emb_ea is embedding of edges\n",
        "        h = model(x, adj_t, edge_attr, emb_ea)\n",
        "        # print(\"Model output shape:\",h.shape)\n",
        "        # edge: we get a sampling of positive edge connections between nodes by using perm(list of nodes for the batch we know are positive)\n",
        "        edge = pos_train_edge[perm].t()\n",
        "        # we pass the edge list inside embedding h\n",
        "        # we devide in edge[0] and edge[1] to devide the connected from->to\n",
        "        pos_out = predictor(h[edge[0]], h[edge[1]])\n",
        "        pos_loss = -torch.log(pos_out + 1e-15).mean()\n",
        "        # neg_sampling(we get negative edges from dataset) REAL ONES! they use the full dataset!!! VERY GOOD :)\n",
        "        edge = negative_sampling(edge_index, num_nodes=x.size(0),\n",
        "                                 num_neg_samples=perm.size(0), method='dense')\n",
        "\n",
        "        neg_out = predictor(h[edge[0]], h[edge[1]])\n",
        "        neg_loss = -torch.log(1 - neg_out + 1e-15).mean()\n",
        "\n",
        "        loss = pos_loss + neg_loss\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(x, 1.0)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        num_examples = pos_out.size(0)\n",
        "        total_loss += loss.item() * num_examples\n",
        "        total_examples += num_examples\n",
        "\n",
        "    return total_loss / total_examples\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, predictor, edge_attr, x, emb_ea, adj_t, split_edge, evaluator, batch_size):\n",
        "    model.eval()\n",
        "    predictor.eval()\n",
        "\n",
        "    h = model(x, adj_t, edge_attr, emb_ea)\n",
        "\n",
        "    pos_valid_edge = split_edge['valid']['edge'].to(x.device)\n",
        "    neg_valid_edge = split_edge['valid']['edge_neg'].to(x.device)\n",
        "    pos_test_edge = split_edge['test']['edge'].to(x.device)\n",
        "    neg_test_edge = split_edge['test']['edge_neg'].to(x.device)\n",
        "\n",
        "    pos_valid_preds = []\n",
        "    for perm in DataLoader(range(pos_valid_edge.size(0)), batch_size):\n",
        "        edge = pos_valid_edge[perm].t()\n",
        "        pos_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    pos_valid_pred = torch.cat(pos_valid_preds, dim=0)\n",
        "\n",
        "    neg_valid_preds = []\n",
        "    for perm in DataLoader(range(neg_valid_edge.size(0)), batch_size):\n",
        "        edge = neg_valid_edge[perm].t()\n",
        "        neg_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n",
        "\n",
        "    pos_test_preds = []\n",
        "    for perm in DataLoader(range(pos_test_edge.size(0)), batch_size):\n",
        "        edge = pos_test_edge[perm].t()\n",
        "        pos_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    pos_test_pred = torch.cat(pos_test_preds, dim=0)\n",
        "\n",
        "    neg_test_preds = []\n",
        "    for perm in DataLoader(range(neg_test_edge.size(0)), batch_size):\n",
        "        edge = neg_test_edge[perm].t()\n",
        "        neg_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    neg_test_pred = torch.cat(neg_test_preds, dim=0)\n",
        "\n",
        "    results = {}\n",
        "    for K in [20, 50, 100]:\n",
        "        evaluator.K = K\n",
        "        valid_hits = evaluator.eval({\n",
        "            'y_pred_pos': pos_valid_pred,\n",
        "            'y_pred_neg': neg_valid_pred,\n",
        "        })[f'hits@{K}']\n",
        "        test_hits = evaluator.eval({\n",
        "            'y_pred_pos': pos_test_pred,\n",
        "            'y_pred_neg': neg_test_pred,\n",
        "        })[f'hits@{K}']\n",
        "\n",
        "        results[f'Hits@{K}'] = (valid_hits, test_hits)\n",
        "\n",
        "    return results\n",
        "\n",
        "def print_gpu_utilization(position):\n",
        "    print(position)\n",
        "    os.system(\"nvidia-smi\")\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Link_Pred_DDI')\n",
        "    parser.add_argument('--device', type=str, default=\"cuda:0\")\n",
        "    parser.add_argument('--num_layers', type=int, default=2)\n",
        "    parser.add_argument('--num_samples', type=int, default=5)\n",
        "    parser.add_argument('--node_emb', type=int, default=256)\n",
        "    parser.add_argument('--hidden_channels', type=int, default=256)\n",
        "    parser.add_argument('--dropout', type=float, default=0.3)\n",
        "    parser.add_argument('--spd_size', type=int, default=500)\n",
        "    # parser.add_argument('--batch_size', type=int, default=1) \n",
        "    # TODO\n",
        "    parser.add_argument('--batch_size', type=int, default=64 * 1024)\n",
        "    parser.add_argument('--lr', type=float, default=0.003)\n",
        "    parser.add_argument('--epochs', type=int, default=400)\n",
        "    parser.add_argument('--log_steps', type=int, default=1)\n",
        "    parser.add_argument('--eval_steps', type=int, default=1)\n",
        "    parser.add_argument('--runs', type=int, default=10)\n",
        "    args = parser.parse_args()\n",
        "    print(args)\n",
        "    device = args.device if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "    \n",
        "\n",
        "    dataset = PygLinkPropPredDataset(name='ogbl-ddi')\n",
        "    data = dataset[0]\n",
        "    print(data.size())\n",
        "    edge_index = data.edge_index.to(device)\n",
        "    print_gpu_utilization(\"Start\")\n",
        "\n",
        "    split_edge = dataset.get_edge_split()\n",
        "\n",
        "    model = GraphSAGE(args.node_emb, args.hidden_channels, args.hidden_channels,\n",
        "                      args.num_layers, args.dropout).to(device)\n",
        "\n",
        "    emb = torch.nn.Embedding(data.num_nodes, args.node_emb).to(device)\n",
        "    emb_ea = torch.nn.Embedding(args.num_samples, args.node_emb).to(device)\n",
        "    predictor = LinkPredictor(args.hidden_channels, args.hidden_channels, 1,\n",
        "                              args.num_layers+1, args.dropout).to(device)\n",
        "\n",
        "    print('Number of parameters:',\n",
        "          sum(p.numel() for p in list(model.parameters()) +\n",
        "          list(predictor.parameters()) + list(emb.parameters()) + list(emb_ea.parameters())))\n",
        "\n",
        "    # encode distance information\n",
        "    np.random.seed(0)\n",
        "    nx_graph = to_networkx(data, to_undirected=True)\n",
        "    node_mask = []\n",
        "    spd_size=args.spd_size\n",
        "    for _ in range(args.num_samples):\n",
        "        node_mask.append(np.random.choice(spd_size, size=int(np.round(spd_size/2.5)), replace=False))\n",
        "    node_mask = np.array(node_mask)\n",
        "    node_subset1 = np.random.choice(nx_graph.number_of_nodes(), size=spd_size, replace=False)\n",
        "    # node_subset2 = np.random.choice(nx_graph.number_of_nodes(), size=250, replace=False)\n",
        "    # TODO\n",
        "    print(\"calling get_spd_matrix\")\n",
        "    spd = get_spd_matrix(G=nx_graph, S=node_subset1, max_spd=5)\n",
        "    # spd2 = get_spd_matrix(G=nx_graph, S=node_subset2, max_spd=5)\n",
        "    print(spd.shape)\n",
        "    # spd=np.concatenate((spd,spd2), axis=1)\n",
        "    torch.cuda.empty_cache()\n",
        "    print(spd.shape)\n",
        "    print(\"after spd\")\n",
        "    spd = torch.Tensor(spd).to(device)\n",
        "    print_gpu_utilization(\"spd loaded\")\n",
        "    edge_attr = spd[edge_index, :].mean(0)[:, node_mask].mean(2)\n",
        "    a_max = torch.max(edge_attr, dim=0, keepdim=True)[0]\n",
        "    a_min = torch.min(edge_attr, dim=0, keepdim=True)[0]\n",
        "    edge_attr = (edge_attr - a_min) / (a_max - a_min + 1e-6)\n",
        "    evaluator = Evaluator(name='ogbl-ddi')\n",
        "    loggers = {\n",
        "        'Hits@20': Logger(args.runs, args),\n",
        "        'Hits@50': Logger(args.runs, args),\n",
        "        'Hits@100': Logger(args.runs, args),\n",
        "    }\n",
        "    for run in range(args.runs):\n",
        "        torch.cuda.empty_cache()\n",
        "        random.seed(run)\n",
        "        torch.manual_seed(run)\n",
        "        torch.nn.init.xavier_uniform_(emb.weight)\n",
        "        torch.nn.init.xavier_uniform_(emb_ea.weight)\n",
        "        model.reset_parameters()\n",
        "        predictor.reset_parameters()\n",
        "        optimizer = torch.optim.Adam(\n",
        "            list(model.parameters()) + list(emb.parameters()) +\n",
        "            list(emb_ea.parameters()) + list(predictor.parameters()), lr=args.lr)\n",
        "\n",
        "        for epoch in range(1, 1 + args.epochs):\n",
        "            loss = train(model, predictor, edge_attr, emb.weight, emb_ea.weight, edge_index, split_edge,\n",
        "                         optimizer,device, args.batch_size)\n",
        "            if epoch % args.eval_steps == 0:\n",
        "                results = test(model, predictor, edge_attr, emb.weight, emb_ea.weight, edge_index, split_edge,\n",
        "                               evaluator, args.batch_size)\n",
        "                for key, result in results.items():\n",
        "                    loggers[key].add_result(run, result)\n",
        "\n",
        "                if epoch % args.log_steps == 0:\n",
        "                    for key, result in results.items():\n",
        "                        valid_hits, test_hits = result\n",
        "                        print(key)\n",
        "                        print(f'Run: {run + 1:02d}, '\n",
        "                              f'Epoch: {epoch:02d}, '\n",
        "                              f'Loss: {loss:.4f}, '\n",
        "                              f'Valid: {100 * valid_hits:.2f}%, '\n",
        "                              f'Test: {100 * test_hits:.2f}%')\n",
        "                    print('---')\n",
        "        torch.save(model,\"save.pth\")\n",
        "        for key in loggers.keys():\n",
        "            print(key)\n",
        "            loggers[key].print_statistics(run)\n",
        "\n",
        "    for key in loggers.keys():\n",
        "        print(key)\n",
        "        loggers[key].print_statistics()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --node_emb 256 --hidden_channels 256 --num_samples 3 --num_layers 2 --epochs 1000 --spd_size 500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJmzOrHQPKld",
        "outputId": "497c36c6-afc7-4b37-9d4c-23f368d201ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=65536, device='cuda:0', dropout=0.3, epochs=1000, eval_steps=1, hidden_channels=256, log_steps=1, lr=0.003, node_emb=256, num_layers=2, num_samples=3, runs=10, spd_size=500)\n",
            "(4267, 4267)\n",
            "Start\n",
            "Wed Apr 27 11:02:12 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P0    28W /  70W |   1290MiB / 15109MiB |      4%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Number of parameters: 1487617\n",
            "calling get_spd_matrix\n",
            "4267\n",
            "spd_for\n",
            "spd_end\n",
            "(4267, 500)\n",
            "(4267, 500)\n",
            "after spd\n",
            "spd loaded\n",
            "Wed Apr 27 11:03:52 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    27W /  70W |   1312MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Hits@20\n",
            "Run: 01, Epoch: 01, Loss: 1.2129, Valid: 0.18%, Test: 0.37%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 01, Loss: 1.2129, Valid: 0.41%, Test: 0.64%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 01, Loss: 1.2129, Valid: 0.68%, Test: 1.06%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 02, Loss: 0.9156, Valid: 2.76%, Test: 5.05%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 02, Loss: 0.9156, Valid: 4.40%, Test: 6.37%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 02, Loss: 0.9156, Valid: 6.36%, Test: 8.36%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 03, Loss: 0.7990, Valid: 1.65%, Test: 0.70%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 03, Loss: 0.7990, Valid: 3.27%, Test: 4.22%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 03, Loss: 0.7990, Valid: 5.14%, Test: 7.12%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 04, Loss: 0.7002, Valid: 3.48%, Test: 5.59%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 04, Loss: 0.7002, Valid: 7.06%, Test: 9.56%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 04, Loss: 0.7002, Valid: 11.28%, Test: 13.35%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 05, Loss: 0.6274, Valid: 8.58%, Test: 4.81%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 05, Loss: 0.6274, Valid: 12.62%, Test: 13.04%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 05, Loss: 0.6274, Valid: 17.12%, Test: 18.30%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 06, Loss: 0.5846, Valid: 11.49%, Test: 10.16%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 06, Loss: 0.5846, Valid: 16.57%, Test: 16.27%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 06, Loss: 0.5846, Valid: 22.31%, Test: 23.16%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 07, Loss: 0.5499, Valid: 14.42%, Test: 6.90%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 07, Loss: 0.5499, Valid: 18.93%, Test: 14.79%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 07, Loss: 0.5499, Valid: 24.17%, Test: 20.24%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 08, Loss: 0.5172, Valid: 17.91%, Test: 9.50%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 08, Loss: 0.5172, Valid: 24.15%, Test: 15.91%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 08, Loss: 0.5172, Valid: 30.58%, Test: 23.34%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 09, Loss: 0.4807, Valid: 25.12%, Test: 13.19%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 09, Loss: 0.4807, Valid: 30.50%, Test: 21.61%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 09, Loss: 0.4807, Valid: 37.08%, Test: 32.76%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 10, Loss: 0.4524, Valid: 21.87%, Test: 8.49%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 10, Loss: 0.4524, Valid: 30.25%, Test: 18.73%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 10, Loss: 0.4524, Valid: 38.92%, Test: 29.87%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 11, Loss: 0.4305, Valid: 19.92%, Test: 11.38%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 11, Loss: 0.4305, Valid: 27.77%, Test: 21.69%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 11, Loss: 0.4305, Valid: 36.46%, Test: 30.59%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 12, Loss: 0.4093, Valid: 20.35%, Test: 15.18%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 12, Loss: 0.4093, Valid: 28.53%, Test: 25.05%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 12, Loss: 0.4093, Valid: 38.12%, Test: 32.45%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 13, Loss: 0.3908, Valid: 23.81%, Test: 18.62%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 13, Loss: 0.3908, Valid: 30.84%, Test: 26.48%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 13, Loss: 0.3908, Valid: 39.15%, Test: 32.26%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 14, Loss: 0.3753, Valid: 25.23%, Test: 18.55%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 14, Loss: 0.3753, Valid: 36.36%, Test: 29.17%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 14, Loss: 0.3753, Valid: 43.08%, Test: 36.55%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 15, Loss: 0.3611, Valid: 21.41%, Test: 14.83%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 15, Loss: 0.3611, Valid: 32.61%, Test: 25.29%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 15, Loss: 0.3611, Valid: 40.03%, Test: 31.84%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 16, Loss: 0.3464, Valid: 26.59%, Test: 20.07%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 16, Loss: 0.3464, Valid: 34.12%, Test: 27.65%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 16, Loss: 0.3464, Valid: 43.28%, Test: 35.71%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 17, Loss: 0.3334, Valid: 30.05%, Test: 19.09%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 17, Loss: 0.3334, Valid: 38.24%, Test: 27.47%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 17, Loss: 0.3334, Valid: 45.69%, Test: 37.63%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 18, Loss: 0.3213, Valid: 31.81%, Test: 21.19%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 18, Loss: 0.3213, Valid: 40.48%, Test: 33.70%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 18, Loss: 0.3213, Valid: 46.91%, Test: 41.73%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 19, Loss: 0.3135, Valid: 29.29%, Test: 19.14%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 19, Loss: 0.3135, Valid: 38.10%, Test: 31.19%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 19, Loss: 0.3135, Valid: 45.92%, Test: 41.50%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 20, Loss: 0.3035, Valid: 35.42%, Test: 27.47%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 20, Loss: 0.3035, Valid: 44.01%, Test: 38.78%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 20, Loss: 0.3035, Valid: 51.12%, Test: 48.09%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 21, Loss: 0.2962, Valid: 31.48%, Test: 16.50%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 21, Loss: 0.2962, Valid: 44.82%, Test: 25.68%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 21, Loss: 0.2962, Valid: 50.06%, Test: 35.69%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 22, Loss: 0.2882, Valid: 28.71%, Test: 16.54%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 22, Loss: 0.2882, Valid: 44.11%, Test: 28.25%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 22, Loss: 0.2882, Valid: 50.11%, Test: 38.54%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 23, Loss: 0.2878, Valid: 37.00%, Test: 17.48%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 23, Loss: 0.2878, Valid: 44.46%, Test: 27.18%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 23, Loss: 0.2878, Valid: 51.22%, Test: 36.99%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 24, Loss: 0.2783, Valid: 40.10%, Test: 20.68%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 24, Loss: 0.2783, Valid: 46.83%, Test: 33.60%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 24, Loss: 0.2783, Valid: 53.56%, Test: 45.13%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 25, Loss: 0.2707, Valid: 41.19%, Test: 17.56%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 25, Loss: 0.2707, Valid: 48.31%, Test: 31.49%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 25, Loss: 0.2707, Valid: 54.44%, Test: 43.62%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 26, Loss: 0.2664, Valid: 32.86%, Test: 14.62%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 26, Loss: 0.2664, Valid: 43.18%, Test: 24.21%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 26, Loss: 0.2664, Valid: 52.45%, Test: 35.29%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 27, Loss: 0.2633, Valid: 37.33%, Test: 15.23%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 27, Loss: 0.2633, Valid: 48.97%, Test: 29.20%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 27, Loss: 0.2633, Valid: 54.66%, Test: 39.21%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 28, Loss: 0.2553, Valid: 34.23%, Test: 16.45%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 28, Loss: 0.2553, Valid: 47.13%, Test: 24.65%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 28, Loss: 0.2553, Valid: 54.06%, Test: 35.49%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 29, Loss: 0.2534, Valid: 41.74%, Test: 25.06%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 29, Loss: 0.2534, Valid: 51.06%, Test: 33.87%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 29, Loss: 0.2534, Valid: 56.60%, Test: 47.08%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 30, Loss: 0.2459, Valid: 44.09%, Test: 15.58%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 30, Loss: 0.2459, Valid: 52.30%, Test: 32.06%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 30, Loss: 0.2459, Valid: 57.68%, Test: 46.07%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 31, Loss: 0.2421, Valid: 45.23%, Test: 14.26%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 31, Loss: 0.2421, Valid: 54.47%, Test: 27.10%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 31, Loss: 0.2421, Valid: 59.01%, Test: 40.08%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 32, Loss: 0.2390, Valid: 47.42%, Test: 11.97%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 32, Loss: 0.2390, Valid: 55.07%, Test: 25.90%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 32, Loss: 0.2390, Valid: 60.06%, Test: 41.30%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 33, Loss: 0.2350, Valid: 50.82%, Test: 19.77%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 33, Loss: 0.2350, Valid: 55.52%, Test: 35.65%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 33, Loss: 0.2350, Valid: 60.00%, Test: 53.34%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 34, Loss: 0.2324, Valid: 48.43%, Test: 17.75%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 34, Loss: 0.2324, Valid: 55.27%, Test: 30.93%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 34, Loss: 0.2324, Valid: 59.62%, Test: 47.80%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 35, Loss: 0.2306, Valid: 48.08%, Test: 14.88%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 35, Loss: 0.2306, Valid: 56.30%, Test: 23.85%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 35, Loss: 0.2306, Valid: 60.75%, Test: 34.71%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 36, Loss: 0.2277, Valid: 48.36%, Test: 14.46%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 36, Loss: 0.2277, Valid: 57.26%, Test: 28.77%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 36, Loss: 0.2277, Valid: 61.55%, Test: 45.16%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 37, Loss: 0.2230, Valid: 53.66%, Test: 15.04%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 37, Loss: 0.2230, Valid: 59.10%, Test: 31.04%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 37, Loss: 0.2230, Valid: 62.43%, Test: 52.34%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 38, Loss: 0.2202, Valid: 48.32%, Test: 12.78%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 38, Loss: 0.2202, Valid: 58.52%, Test: 23.37%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 38, Loss: 0.2202, Valid: 61.87%, Test: 41.44%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 39, Loss: 0.2196, Valid: 50.18%, Test: 15.44%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 39, Loss: 0.2196, Valid: 59.13%, Test: 28.57%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 39, Loss: 0.2196, Valid: 63.05%, Test: 55.31%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 40, Loss: 0.2180, Valid: 53.08%, Test: 14.99%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 40, Loss: 0.2180, Valid: 59.10%, Test: 38.09%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 40, Loss: 0.2180, Valid: 62.82%, Test: 67.35%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 41, Loss: 0.2180, Valid: 53.76%, Test: 14.93%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 41, Loss: 0.2180, Valid: 60.08%, Test: 28.32%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 41, Loss: 0.2180, Valid: 63.98%, Test: 54.11%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 42, Loss: 0.2114, Valid: 54.11%, Test: 14.81%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 42, Loss: 0.2114, Valid: 61.02%, Test: 27.89%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 42, Loss: 0.2114, Valid: 64.45%, Test: 55.79%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 43, Loss: 0.2099, Valid: 51.77%, Test: 9.20%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 43, Loss: 0.2099, Valid: 59.30%, Test: 27.24%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 43, Loss: 0.2099, Valid: 63.96%, Test: 54.91%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 44, Loss: 0.2077, Valid: 56.67%, Test: 18.32%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 44, Loss: 0.2077, Valid: 61.69%, Test: 41.37%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 44, Loss: 0.2077, Valid: 65.21%, Test: 67.03%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 45, Loss: 0.2054, Valid: 54.21%, Test: 9.43%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 45, Loss: 0.2054, Valid: 61.40%, Test: 33.80%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 45, Loss: 0.2054, Valid: 64.86%, Test: 66.30%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 46, Loss: 0.2038, Valid: 56.79%, Test: 10.17%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 46, Loss: 0.2038, Valid: 62.39%, Test: 38.29%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 46, Loss: 0.2038, Valid: 66.31%, Test: 73.44%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 47, Loss: 0.2016, Valid: 49.77%, Test: 12.07%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 47, Loss: 0.2016, Valid: 59.81%, Test: 35.47%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 47, Loss: 0.2016, Valid: 64.95%, Test: 63.78%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 48, Loss: 0.2021, Valid: 56.81%, Test: 15.16%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 48, Loss: 0.2021, Valid: 62.89%, Test: 41.82%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 48, Loss: 0.2021, Valid: 66.64%, Test: 71.82%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 49, Loss: 0.2006, Valid: 56.67%, Test: 19.65%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 49, Loss: 0.2006, Valid: 63.29%, Test: 51.38%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 49, Loss: 0.2006, Valid: 66.57%, Test: 73.69%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 50, Loss: 0.1966, Valid: 58.13%, Test: 26.08%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 50, Loss: 0.1966, Valid: 63.77%, Test: 61.94%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 50, Loss: 0.1966, Valid: 66.28%, Test: 78.18%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 51, Loss: 0.1964, Valid: 56.77%, Test: 33.20%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 51, Loss: 0.1964, Valid: 63.62%, Test: 60.09%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 51, Loss: 0.1964, Valid: 66.38%, Test: 78.62%\n",
            "---\n",
            "Hits@20\n",
            "Run: 01, Epoch: 52, Loss: 0.1954, Valid: 56.31%, Test: 38.81%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 52, Loss: 0.1954, Valid: 62.97%, Test: 67.22%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 52, Loss: 0.1954, Valid: 66.50%, Test: 78.60%\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}